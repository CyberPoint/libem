/* EXPECTATION MAXIMIZATION ALGORITHM
   Written by Elizabeth Garbee
   CyberPoint International, LLC 2012 */

#include <iostream>
#include <cmath>

/* Given a collection of inner products, we want to estimate their distribution using a mixture of N weighted Gaussians. Obviously, we want to choose those Gaussians that result in the closest fit to the true distribution of the inner products. This requires us to not only compute the means and variances of the N Gaussians, but also to determine for each data point how responsible each of the N Gaussians was in generating it.

The parameters we want to estimate are beta = (beta1 ... betaN), mu = (mu1 ... muN) and sigma = (sigma1 ... sigmaN): theta = (beta, mu, sigma). Supposing we have a data matrix U in M(K x N), we want to compute the matrix J in M(K x N) where each Ji,j represents the probability that the ith data point was generated by the jth Gaussian. The matrix J is analogous to the "hidden data" in the model.

We're not interested in the values of the hidden variables - we just want to maximize the probability of our estimate (theta') given the data (U). That is, we want to compute the value of theta which maximizes the log-likelihood. This allows us to express the EM algorithm as follows:
given an initial estimate of theta (theta0), we repeatedly perform two steps until our estimates of theta converge:

	Estep: compute q^(t + 1) = argmaxq L(q, theta^t)	
	Mstep: compute theta^(t + 1) = argmaxtheta L(q^(t + 1), theta)

Successive estimates of theta taken this way will eventually converge to the local max. */
#define WIDTH K
#define HEIGHT N

int pi = 3.14159264;
int beta;
int beta_next;
int mu;
int mu_next;
int sigma;
int sigma_next;
int theta = (beta, mu, sigma);
int theta0;
int U = [WIDTH][HEIGHT]
int J;

int Estep(int U, int theta, int J, int j, int r, double z, double numerator, double denominator) 
{
	using std::cout;

	int J = 0;
	int r;

	for (int i = 1; i < K; i++)
	do
		for (int j = 1; j < K; j++)
		do	
			double z = (1.0 / pow((sqrt(2.0*pi)), N) * 
			double numerator = 
			double denominator = 

			for (int r = 1; r < K; r++)
}


int Mstep(int U, int J)
{
	int s;	

	using std::cout;

	for (int s = 1; s < [WIDTH]; s++)
	do
}


int Kmeans()
{
	using std::cout;
}


int main(int theta_last, int J, int i, int I, )
{
	int theta_last;
	int I = 20; // this is the number of iterations; can be modified from a static number to a 			    condition of convergence
	
	using std::cout;

	for (i = 1; i < I; i++)
	{
		J = Estep(U, theta_last)
		theta = Mstep(U, J)
		cout << " log-likelihood is: " << theta << endl;
	}
return 0;
}

/* E step

Data: U, theta = (beta, mu, sigma)
J = 0
for i in {1 .. K}
do
	for j in {1 .. N}
	do
		z = ( 1 / (sqrt(2*pi))^N * (sqrt(det(sigma over J)) ) * exp(-0.5(U over i - mu over j) ^T * (sigma over J)^-1 * (U over i - mu over j)
		numerator = beta over j * z
		denominator = 0
		for r in {1 ... N} 
		do
			z = (all that above except sigma and mu over r)
			denominator = denominator + beta over r * z
		end
	J = numerator / denominator
	end
end
return J

**M step

Data: U, J
for s in {1 .. N}
do
	beta to the plus one = sigma over j = 1 to K of J
	mu " = sigma over j = 1 to K of UjJj / sigma " of J
	sigma " = sigma(U - mu to the plus one)(U - mu to the plus one)^T * J / sigma over j=1 to K 			of J	
end
return theta to the plus one = (theta to the plus one, mu ", sigma ")

**EM algorithm

Data: U, theta not
for i in {i .. I}
do
	J = E step(U, theta to the minus one)
	theta to i = M step(U, J)
end
return theta' = theta to the I

**K means

Choose C random data points as the cluster means(mu 1 .. mu C)
for i in {1 .. I}
do
	for Uj in U
	do
		assign Uj to the cth cluster such that (Uj - mu c) is minimized
	end
	compute the new cluster means from the assignments just generated
end
return mu

