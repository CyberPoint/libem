#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <cmath>
#include <vector>
#include <iterator>
#define I 1000
// define the number of iterations

using namespace std;


int main()
{
	// initiate a vector U of unspecified numbers of elements to store the data (gaussian mixtures)		
	//vector <float> U;
	// input data into into the newly created vector U
	cout << "Make sure you've added your csv file in the header of this program. \n"; 
	// reading and parsing the CSV data
	ifstream in("test_data.txt");
	string line, field;
	vector < vector<string> > array; // the 2 D array
	vector<string> v; // array of values for one line only

	while(getline(in, line)) // get next line in file
	{
		v.clear();
		stringstream ss(line);
		while(getline(ss, field, ',')) // break line into comma delimitted fields
		{
			v.push_back(field); // add each field to the 1 D array
		}
		array.push_back(v); // add the 1 D array to the 2 D array
	}
	/*//print out what was read in
	for (size_t i = 0; i<array.size(); ++i)
	{
		for (size_t j = 0; j<array[i].size(); ++j)
		{
			cout << array[i][j] << "| \n"; // separate fields with a pipe
		}
	}
	return 0;*/

	// seeding the K means algorithm with either a user input number of clusters, or a number the algorithm intuits 
	char answer;
	char C;
	// give the user options as to how the algorithm chooses initial guesses for the cluster means
	cout << "Would you like to input the initial cluster means (y=Yes/n=No)? ";
	cin >> answer;

	if(answer == 'y')
	{
		// now the user chooses the initial guesses for the means 
		cout << "\n Please enter the number of desired cluster means: ";
		cin >> C;
		int * mu;
		mu = new int [C];
		cout << "\n Please input the values for your chosen cluster means: \n";
		
		
	}
	else
		cout << "The K means algorithm will now choose the initial cluster means: \n";
	return 0;







}













































/*************************************************************************************

The aim of the approximation is as follows - given a collection of inner products, we want to estimate their distribution using a mixture of N weighted gaussians. We want to choose those gaussians that result in the closest fit to the true distribution of the inner products. This requires us to not only compute the means and variances of the N gaussians, but also to determine for each data point how responsible each of the gaussians was in generating it.

	Beta is the weight
	Mu is the mean
	Sigma is the covariance
	use Theta to represent the set of unknowns (beta, mu, sigma)

However, in order to estimate these parameters, we will also need to determine how responsible each of our gaussians was in generating each data point.

Supposing we have a data matrix U with K data vectors, each of which is D dimensional, we want to compute the matrix J, where each J i,j represents the 	probability that the ith data point was generated by the jth gaussian. We want to compute the value of theta which maximizes the log probability of U given theta. 

Given an initial estimate of theta (theta0), we repeatedly perform two steps until the estimates of theta converge.

************************************************************************************

1. seed the algorithm using K means
-initially, choose C random data points as the means for the gaussians' clusters
-for each iteration starting at one and terminating at the last iteration
	-for each U j in U
		-assign U j to the cth cluster such that U j - mu c is minimized
	-compute the new cluster means from the assignments just generated
-return the mean mu


-For each iteration starting at one and terminating at the last iteration
-do
	1. compute the matrix J: J = E-step result of the data matrix and previous 		parameters
		-(as data, use the data matrix U and the parameters theta)
		-initialize J as zero (J = 0)
		-for each data point from 1 to the number of data points in the set
			-for each gaussian from 1 to the number of gaussians
			-initialize a variable z, which represents the actual log 				likelihood calculation
			-use that log likelihood to calculate the probability that 				this data point was contributed to by that gaussian 				(numerator); denominator at this point is zero
			-for each gaussian from 1 to N
				-do the same calculation 
				-the denominator is now the weight times the 					likelihood calculation (denominator + beta * z)
			-calculate j as the numerator/denominator - the probability 				that this gaussian contributed to this data point over the 				total probability
		-return J
			J is the actual log-likelihood for the rest of the algorithm


	2. compute the new parameters: theta = M-step result of the data matrix and 		probability matrix
		-(as data, use the data matrix U and the probability matrix J)
		-for each gaussian from 1 to N (represented by the variable s)
			-calculate the new weight
				the new weight is the sum over j from 1 to N of the 					J matrix
					the sum over all the gaussians from one to 						N of their respective probabilities
			-calculate the new mean
				the new mean is the sum over j from 1 to K of U*J/J
					the data matrix times the probability 						matrix, divided by the probability matrix
			-calculate the new covariance
				the new covariance is the sum over j from 1 to K of 					U minus new mu * U minus new mu transpose * J / J
					the difference between the data set and the 						new mean times the difference between the 						data set and the new mean transposed, times 						the probability matrix / the probability 						matrix
		-return the new parameters
			theta prime is the final guess for the parameters of the 				unknown gaussian




-end
-return the new parameters */


