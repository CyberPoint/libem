<!-- This comment will put IE 6, 7 and 8 in quirks mode -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>EM: Expectation Maximization Algorithm</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javaScript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body onload='searchBox.OnSelectItem(0);'>
<!-- Generated by Doxygen 1.6.1 -->
<script type="text/javascript"><!--
var searchBox = new SearchBox("searchBox", "search",false,'Search');
--></script>
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li class="current"><a href="index.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="annotated.html"><span>Data&nbsp;Structures</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <img id="MSearchSelect" src="search/search.png"
             onmouseover="return searchBox.OnSearchSelectShow()"
             onmouseout="return searchBox.OnSearchSelectHide()"
             alt=""/>
        <input type="text" id="MSearchField" value="Search" accesskey="S"
             onfocus="searchBox.OnSearchFieldFocus(true)" 
             onblur="searchBox.OnSearchFieldFocus(false)" 
             onkeyup="searchBox.OnSearchFieldChange(event)"/>
        <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
        </div>
      </li>
    </ul>
  </div>
</div>
<div class="contents">
<h1>Expectation Maximization Algorithm</h1><h3>v1.0 </h3><h2><a class="anchor" id="intro_sec">
Introduction</a></h2>
<p>In statistics, a mixture model is a probabilistic model for representing the sub-populations within a population, without requiring that the observed data set should itself identify its own sub- populations. Formally, a mixture model corresponds to the mixture distribution representing the probability distribution of observations in the overall population - thus mixture models are used to make inferences about the properties of these sub-populations given only information about the data set as a whole. These are sometimes referred to as the "hidden" parameters of the data set.</p>
<p>Expectation Maximization (EM) is perhaps the most popular technique for discovering the parameters of a mixture with an a priori given number of components. Here's the usual scenario: you have N data points in an M-dimensional space. Your goal is to "fit" this data by finding a set of K multivariate Gaussian distributions that best represent to observed distribution of the data points. You fix the number of distributions (K) in advance, but the means and covariances of these distributions are unknown - the "hidden" parameters of this particular data set. This exercise falls under the category of "unsupervised" learning because you don't know ahead of time which of the N data points come from which of the K Gaussians. And indeed, one of the desired outputs of this approximation is, for each data point n, an estimate of the probability that it could belong to a specific distribution k. This probability is denoted P(n|k). Thus, given the data points stored (for example) as an N x M matrix whose rows are vectors of length M, there are three specific parameters EM will estimate: the K means, each a vector of length M (mu); the K covariance matrices, each of size M x M (sigma); and the P(n|k)s. We will also receive a few additional estimates as by-products of this calculation - the fraction of all the data points belonging to k (denoted P(k)), the probability (actually probability density) of finding some random data point at position x (denoted P(x)), and the overall likelihood of a given estimated parameter set (denoted L).</p>
<p>The likelihood is actually the most important approximated parameter for the overall procedure of EM. L is defined, as usual, as proportional to the probability of the data set, given all the fitted parameters - and we find the best values for these parameters by maximizing L. In statistics, this is similar to maximizing the posterior probability of the parameters, given uniform or even just very broad priors.</p>
<p>An overall outline of the EM calculations is best described by working backwards from L. Since the data points are (assumed) to be independent, L is the product of the probabilities of finding a point at each observed position. We can split that probability density function (sometimes called the mixture weight of the data point) into its contributions from each of the K Gaussians, giving the individual probabilities (denoted Pnk's). In the language of EM, these two calculations of L and Pnk's are called the expectation step, or E step. But you may be asking yourself - those sound lovely, but where are the mu's, sigma's and P(k)'s I was promised in the beginning? Suppose we "knew" the Pnk's. If you are familiar with the one-dimensional Gaussian distribution, you will be familiar with the concept of the maximum liklihood estimate, in which the mean of a given Gaussian is given as just the arithmetic mean of a set of points drawn from it. This generalizes to give maximum likelihood estimates for the means (mu's) and covariance matrices (sigma's) of multivariate Gaussians. A further generalization is that, since we know only whether a particular point comes from a particular Gaussian (the Pnk's), we should count only the appropriate fraction of each point. These maximum likelihood estimates of mu and sigma are called the maximization step, or M step.</p>
<p>The particular power of the EM algorithm comes from a more powerful theorem (beyond our scope to prove here) stating that, starting from any parameter values, an iteration of E step followed by an M step will always increase the likelihood value of L; and that repeated iterations between these two steps will converge to (at least) a local maximum. Often, the convergence is indeed to the global maximum. The EM algorithm, in brief, goes like this:</p>
<ul>
<li>Guess starting values for the mu's, sigma's and P(k)s for each of your Gaussians</li>
<li>Repeat: an E step to get new L's and Pnk's, followed by an M step to get new mu's, sigma's and P(k)s</li>
<li>Stop when the value of L is no longer changing, which in this scope defines convergence</li>
</ul>
<p>In this particular implementation of EM, we use the clustering algorithm Kmeans to provide the initial guesses for the means of the K Gaussians, in order to increase the efficiency and efficacy of EM.</p>
<p>One important detail to note is that often, the values of the Gaussian density function will be so small as to underflow to zero. Therefore, it is very important to work with the logarithms of these densities, rather than the densities themselves. This particular implementation works in log space in an attempt to avoid this issue.</p>
<p>NOTE: The <a class="el" href="classMatrix.html">Matrix</a> class used extensively in the EM library was written by Riva Borbley, and only edited slightly by Elizabeth Garbee.</p>
<h2><a class="anchor" id="usage_sec">
Usage</a></h2>
<p>The following is a workflow outline of sorts, detailing the steps necessary to successfully implement this EM algorithm.</p>
<ul>
<li>1. Install BLAS and LAPACK on your machine if they're not already there.</li>
<li>2. Update the makefile/change the <a class="el" href="EM__Algorithm_8cpp.html">EM_Algorithm.cpp</a> header to point to where you downloaded this code and your copies of BLAS and LAPACK are - this is specific to each box.</li>
<li>3. Edit <a class="el" href="EM__Algorithm_8h.html">EM_Algorithm.h</a> to point to the correct location of <a class="el" href="Matrix_8h.html">Matrix.h</a>.</li>
<li>4. make</li>
<li>5. ./em_algorithm &lt;data_file&gt; &lt;num_dimensions&gt; &lt;num_data_points&gt; &lt;num_clusters&gt;</li>
</ul>
<h2><a class="anchor" id="example_sec">
Caller Example</a></h2>
<p>For an example "main" that will run EM, see the README. </p>
</div>
<!--- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&nbsp;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&nbsp;</span>Data Structures</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&nbsp;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&nbsp;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&nbsp;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&nbsp;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&nbsp;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&nbsp;</span>Defines</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<hr size="1"/><address style="text-align: right;"><small>Generated on 16 Aug 2012 for EM by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.6.1 </small></address>
</body>
</html>
