<!-- This comment will put IE 6, 7 and 8 in quirks mode -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>EM: libGaussMix++: An Expectation Maximization Algorithm for Training Gaussian Mixture Models</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javaScript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body onload='searchBox.OnSelectItem(0);'>
<!-- Generated by Doxygen 1.6.1 -->
<script type="text/javascript"><!--
var searchBox = new SearchBox("searchBox", "search",false,'Search');
--></script>
<div class="navigation" id="top">
  <div class="tabs">
    <ul>
      <li class="current"><a href="index.html"><span>Main&nbsp;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Data&nbsp;Structures</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <img id="MSearchSelect" src="search/search.png"
             onmouseover="return searchBox.OnSearchSelectShow()"
             onmouseout="return searchBox.OnSearchSelectHide()"
             alt=""/>
        <input type="text" id="MSearchField" value="Search" accesskey="S"
             onfocus="searchBox.OnSearchFieldFocus(true)" 
             onblur="searchBox.OnSearchFieldFocus(false)" 
             onkeyup="searchBox.OnSearchFieldChange(event)"/>
        <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
        </div>
      </li>
    </ul>
  </div>
</div>
<div class="contents">
<h1>libGaussMix++: An Expectation Maximization Algorithm for Training Gaussian Mixture Models</h1><h3>v1.0 </h3><dl class="author"><dt><b>Authors:</b></dt><dd>Elizabeth Garbee, James Ulrich, CyberPoint Labs </dd></dl>
<dl class="date"><dt><b>Date:</b></dt><dd>October 15, 2012</dd></dl>
<p>Copyright 2012 CyberPoint International LLC.</p>
<p>The contents of libGaussMix++ are offered under the NewBSD license:</p>
<p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: (1) Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. (2) Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. (3) Neither the name of the CyberPoint International, LLC nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</p>
<p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL CYBERPOINT INTERNATIONAL, LLC BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>
<h2><a class="anchor" id="intro_sec">
Introduction</a></h2>
<p>A Gaussian mixture model is method of approximating, as a linear combination of multivariate Gaussian density functions, an unknown density function from which a given data set is presumed to be drawn. The means and covariance matrices of these Gaussian densities are sometimes referred to as the "hidden" parameters of the data set.</p>
<p>Expectation Maximization (EM) is perhaps the most popular technique for discovering the parameters of a mixture with a given number of components. Here's the usual scenario: one is given a set of N data points in an M-dimensional space. One seeks to fit to the data a set of k multivariate Gaussian distributions (or "clusters") that best represent the observed distribution of the data points, where k is specified, but the means and covariances of the Gaussians are unknown. The desired output includes the means and covariances of the Gaussians, along with the cluster weights (coefficients of the linear combination of Gaussians). One may think of the data as being drawn by first randomly choosing a particular Gaussian according to the cluster weights, and then choosing a data point according to the selected distribution. The overall probability density assigned to a point is then given by the sum over k of P(n|k)P(k) where k denotes the cluster, P(k) its probability, and P(n|k) denotes the probability density of the point given the cluster. The likelihood density L of the data is the product of these sums, taken over all data points (which are assumed to be independent and identically distributed). The EM algorithm produces an estimate of the P(k) and the P(n|k),by attempting to maximize L. Specifically, it yields the estimated means mu(k), covariance matrices sigma(k), and weights P(k) for each Gaussian.</p>
<p>An outline of the EM calculations is best described by working backwards from L. Since the data points are (assumed) to be independent, L is the product of the probability densities of each observed data point, which splits into a contribution P(n|k) from each Gaussian (these contributions are sometimes called the data point "mixture weights"). In the language of EM, the L and P(n|k) calculations, given known mu(k), sigma(k), and P(k), comprise the algorithm's "expectation step", or E step. Now if the P(n|k)'s are known, we can derive from them maximum likelihood estimates of the mu(k), sigma(k), and P(k), in a process called the "maximization step", or M step, from which we then obtain a new set of P(n|k), and a new (better) L.</p>
<p>The power of the EM algorithm comes from a theorem (see the first reference) stating that, starting from any reasonable guess of parameter values, an iteration of an E step followed by an M step will always increase the likelihood value L, and that repeated iterations between these two steps will converge to (at least) a local maximum. Often, the convergence is indeed to the global maximum. The EM algorithm, in brief, goes like this:</p>
<ul>
<li>Guess starting values for the mu(k), sigma(k) and P(k) for each Gaussian,</li>
<li>Repeat: an E step to get a new L and new P(n|k)s, followed by an M step to get new mu(k)s, sigma(k)s, and P(k)s,</li>
<li>Stop when the value of L is no longer meaningfully changing.</li>
</ul>
<p>The libGaussMix++ implementation of the EM algorithm uses the "KMeans" clustering algorithm to provide the initial guesses for the means of the K Gaussians, in order to increase the efficiency and efficacy of EM.</p>
<p>The libGaussMix++ code base also includes support for adapation of a GMM to a specific subpopulation of the population on which it was trained. In a sense, this "biases" the model towards the subpopulation. If the population splits into distinct subpopulations, one may then classify a sample as belonging to a particular subpopulation, by assiging it to the subpopulation under whose adapted GMM it has the highest likelihood density. One often normalizes these scores by the unadapted GMM density. See reference 2 for details. This functionality is provided by the <a class="el" href="namespacegaussmix.html#af592c6765ff28a99d17cacf87a608a21" title="adapt: adapt a Gaussian Mixture model to a given sub-population.">adapt()</a> method of the GaussMix API.</p>
<h2><a class="anchor" id="usage_sec">
Usage</a></h2>
<p>libGaussMix++ does not actually build as a library. The API is provided in <a class="el" href="GaussMix_8h.html" title="Function prototypes for libGaussMix++ routines.">GaussMix.h</a>; API implementations are contained in <a class="el" href="GaussMix_8cpp.html" title="core libGaussMix++ em algorithm method implementations.">GaussMix.cpp</a>. The implementations use helper routines provided in <a class="el" href="Matrix_8h.html" title="Definitions for a Matrix class wrapping lapack/blas matrix routimes.">Matrix.h</a>/ <a class="el" href="Matrix_8cpp.html" title="Matrix class method implementations.">Matrix.cpp</a>, which in turn wrap LAPACK linear algebra functions, and in <a class="el" href="KMeans_8h.html" title="definitions for kmeans clustering algorithm">KMeans.h</a>/ <a class="el" href="KMeans_8cpp.html" title="implementations for kmeans clustering algorithm">KMeans.cpp</a>, to generate initial model parameter guesses. To build these files into an executable with a sample driver function provided in <a class="el" href="sample__main_8cpp.html" title="sample main file to show exercise of EM routines.">sample_main.cpp</a>, follow these steps:</p>
<ul>
<li>1. Install BLAS, LAPACK, and LAPACKE on your machine if they're not already there (c.f. <a href="http://www.netlib.org/lapack/,">http://www.netlib.org/lapack/,</a> which bundles a reference vesion of BLAS).</li>
<li>2. Update the environment variables in the libGaussMix++ makefile to point to your environment's BLAS and LAPACK header and library locations.</li>
<li>3. run make on the libGaussMix++ makefile.</li>
<li>4. run the resulting executable via: "gaussmix &lt;data_file&gt; &lt;num_dimensions&gt; &lt;num_data_points&gt; &lt;num_clusters&gt;". <br/>
 Try using one of the sample *.csv or *.svm data files as the first argument: e.g. "gaussmix multid_data_1.csv 3 20 2".</li>
</ul>
<p>The libGaussMix++ source code has been built and tested on SL6.2 using gcc 4.4.5.</p>
<h2><a class="anchor" id="example_sec">
Caller Example</a></h2>
<p>For an example "main" that invokes the libGaussMix++ API routines, see <a class="el" href="sample__main_8cpp.html" title="sample main file to show exercise of EM routines.">sample_main.cpp</a>.</p>
<h2><a class="anchor" id="Notes">
Notes</a></h2>
<p>1. Per the first reference, the values of the Gaussian density functions may be so small as to underflow to zero, and therefore, it is desirable to perform the EM algorithm in the log domain. This implementation takes that approach.</p>
<p>2. libGaussMix++ supports csv and svm-style data formats (c.f. www.csie.ntu.edu.tw/~cjlin/libsvm/). Sample files (multid_data_1.csv and multid_data_2.svm, respectively) are provided for convenience. The former consists of 20 three-dimensional data points falling into two clusters centered on (1,1,1) and (100,100,100). The second is similarly distributed, with a "+1" cluster centered on (10,10,10) and "-1" cluster centered on (50,50,50).</p>
<p>3. The <a class="el" href="Matrix_8h.html" title="Definitions for a Matrix class wrapping lapack/blas matrix routimes.">Matrix.h</a>/ cpp code wraps LAPACK routines. To work with these routines efficiently, the <a class="el" href="classMatrix.html">Matrix</a> class maintains internal buffers that may be modified by operations that, from a semantic point of view, are read-only. Hence many libGauss routines take non-const <a class="el" href="classMatrix.html">Matrix</a> arguments, where const <a class="el" href="classMatrix.html">Matrix</a> arguments would be typically be used.</p>
<p>4. For compilers that support it, the code is configured to use open mp (www.openmp.org) to parallelize various for loops, where the loop is over the cluster of the Gaussian Mixture Model. </p>
<h2><a class="anchor" id="References">
References</a></h2>
<p>1. Press, et. al., Numerical Recipes 3rd Edition: The Art of Scientific Computing, Cambridge University Press New York, NY, USA ©2007, ISBN:0521880688 9780521880688 (c.f. chapter 16).</p>
<p>2. Douglas A. Reynolds, et. al., "Speaker Verication Using Adapated Gaussian Mixture Models," Digital Signal Processing 10, 19-24 (2000). </p>
</div>
<!--- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&nbsp;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&nbsp;</span>Data Structures</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&nbsp;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&nbsp;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&nbsp;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&nbsp;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&nbsp;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&nbsp;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&nbsp;</span>Defines</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<hr size="1"/><address style="text-align: right;"><small>Generated on 7 Feb 2013 for EM by&nbsp;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.6.1 </small></address>
</body>
</html>
