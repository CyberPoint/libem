
   EXPECTATION MAXIMIZATION ALGORITHM (readme)
   CYBERPOINT INTERNATIONAL, LLC
   Written by Elizabeth Garbee, Summer 2012 

REFERENCE:

Numerical Recipes 3rd Edition: The Art of Scientific Computing 3 
Cambridge University Press New York, NY, USA Â©2007 
ISBN:0521880688 9780521880688

DESCRIPTION:

Gaussian Mixture Models are some of the simplest examples of classification for unsupervised learning - they are also some of the simplest examples where a solution by an Expectation Maximization algorithm is very successful. Here's the setup: you have N data points in an M dimensional space. You want to fit this data (and if you don't, you might as well stop reading now), but in a special sense - find a set of K multivariate Gaussian distributions that best represents the observed distribution of data points. K is fixed in advance but the means and covariances are unknown. What makes this "unsupervised" learning is that you have "unknown" data, which in this case are the individual data points' cluster memberships. One of the desired outputs of this algorithm is for each data point n, an estimate of the probability that it came from distribution number k. Thus, given the data points, there are three parameters we're interested in approximating:

	mu - the K means, each a vector of length M
	sigma - the K covariance matrices, each of size M x M
	P(k) - the K probabilities for each of N data points

We also get some fun stuff as by-products: the probability density of finding a data point at a certain position x, where x is the M dimensional position vector; and L, the overall likelihood of your estimated parameter set. L is actually the key to the whole problem, because you find the best values for the parameters by maximizing the likelihood of L. This particular implementation of EM actually first implements a Kmeans approximation to provide an initial guess for cluster centroids, in an attempt to improve the precision and efficiency of the EM approximation.

Here's the general procedure:
	-run a kmeans approximation on your data to give yourself a good starting point for your cluster centroids
	-guess starting values for mu's, sigmas, and P(k)'s
	-repeat: an Estep to get new p's and a new L, then an Mstep to get new mu's, sigmas and P(k)'s
	-stop when L converges (i.e. when the value of L stops changing)

Be aware: the majority of the calculations in the functions "estep" and "mstep" are done in log space to avoid underflow.

*This library is compatible with multidimensional, comma delineated data*

DEPENDENCIES:

	BLAS
	LAPACK
	gcc 4.4.5 or greater (code has been tested on SL6.2)
        (code has been tested on LAPACKE 3.4 and appears to operate with OpenMP

USER INSTRUCTIONS:

1. Install BLAS and LAPACK on your machine if they're not already there
2. Update the makefile/change the EM_Algorithm.cpp header to point to where my code and your copies of BLAS and LAPACK are - this is specific to each box.
3. make
4. ./em_algorithm <data_file> <num_dimensions> <num_data_points> <num_clusters>

EXAMPLE CSV DATA:

The following data contains 20 3 dimensional data points, drawn from two distinct gaussian mixtures, with mean vectors of [100 100 100] and [1 1 1]. Notice that there is a comma after each dimension of each data point, with no white space between the digits and the commas. Also make sure that if you enter "20" as your command line argument for the n parameter, you don't have any extra white space above or below your data chunk - ParseCSV will throw up its hands if you tell it there are 20 data points and you have white space on the 21st line.

If your data doesn't look like this or similar, I would suggest writing a routine similar to EM's ParseCSV function that handles your unique format.

99.90536353,100.9519929,101.9385873,
99.54191165,99.80947901,100.9722583,
100.3121106,102.4358085,101.562499,
100.6186488,100.1757651,99.1680364,
100.1469994,100.6456827,100.5566867,
100.65789,99.25896006,100.2776279,
99.07120212,101.2017697,101.3296947,
99.3213353,99.44780776,101.0659122,
98.95710325,99.30719897,99.12198086,
100.4995072,100.7442414,100.286113,
1.215281921,1.062221202,-0.252055442,
0.50444074,0.561117871,1.538166245,
1.98566035,1.61902389,2.187281321,
1.178454094,1.477791732,1.32290785,
0.67922562,1.160030889,2.268453499,
1.204915964,0.848708198,-0.238638331,
1.449834188,0.877311912,-0.232334496,
0.271191307,1.32616999,0.574710975,
2.456305245,2.080303551,1.906343256,
1.876181052,2.223077532,2.909945178,

note: the code can also except data in "libsvm" format:
<label> 1:<data_point_1> 2:<data_point_2> etc. for example:

1 1:99.90536353 2:100.9519929 3:101.9385873
1 1:99.54191165 2:99.80947901 3:100.9722583
1 1:100.3121106 2:102.4358085 3:101.562499 

the labels will be ignored.

EXAMPLE MAIN:

the software ships with a sample_main.cpp example file, to illustrate use of the EM algorithm.
